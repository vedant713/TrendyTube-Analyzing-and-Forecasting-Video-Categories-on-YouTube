{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lnMLgnvpYMc8"
      },
      "outputs": [],
      "source": [
        "dataset_path = \"<Dataset Folder Path Present on Drive>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "EldWrbU1YfG2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_33420\\4051890243.py:12: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
            "  from IPython.core.display import HTML,display\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections,os,json\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from IPython.core.display import HTML,display\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "import xgboost\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from warnings import simplefilter\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "simplefilter(action='ignore', category=UserWarning)\n",
        "simplefilter(action='ignore', category=RuntimeWarning)\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eS6hCWO_Yg4a"
      },
      "outputs": [],
      "source": [
        "def csspath():\n",
        "  return display(HTML(\"\"\"<style>\n",
        "  #output-body {\n",
        "    display: flex;\n",
        "    align-items: center;\n",
        "    justify-content: center;\n",
        "    background-color:white;\n",
        "    color:black;\n",
        "    }\n",
        "  .output_text pre{\n",
        "    margin-right : 70px;\n",
        "    color: black;\n",
        "    font-weight: bold;\n",
        "    /* font-size: 10px; */\n",
        "  }\n",
        "\n",
        "}  </style>\"\"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oExDVsQYmta"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5_yWwrODSFI"
      },
      "source": [
        "#Data Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6vZNRi91vMs"
      },
      "source": [
        "##Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z6p70r9Rot8Q"
      },
      "outputs": [],
      "source": [
        "def dateprocessattributes(data):\n",
        "  data[\"publish_time\"] = pd.to_datetime(data[\"publish_time\"])\n",
        "  data[\"trending_date\"] = pd.to_datetime(data[\"trending_date\"],format=\"%y.%d.%m\")\n",
        "  data[\"trend_year\"]=data[\"trending_date\"].apply(lambda time:time.year)\n",
        "  data[\"trend_date\"] = data[\"trending_date\"].apply(lambda time: time.day)\n",
        "  data[\"trend_month\"]=data[\"trending_date\"].apply(lambda time:time.month)\n",
        "  data[\"publish_year\"]=data[\"publish_time\"].apply(lambda time:time.year)\n",
        "  data[\"publish_month\"]=data[\"publish_time\"].apply(lambda time:time.month)\n",
        "  data[\"publish_day\"]=data[\"publish_time\"].apply(lambda time:time.day)\n",
        "  data[\"publish_weekday\"]=data[\"publish_time\"].apply(lambda time:time.dayofweek)\n",
        "  data[\"publish_hour\"]=data[\"publish_time\"].apply(lambda time:time.hour)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "m_VzmTLBBdn2"
      },
      "outputs": [],
      "source": [
        "def categmatch(x,dictionary): #function to get the categories\n",
        "    try:\n",
        "        return dictionary[str(x)]\n",
        "    except:\n",
        "        return \"Unknown\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-OkorSq10s1"
      },
      "source": [
        "##Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "3mpQIOPz10Ws"
      },
      "outputs": [],
      "source": [
        "def cleandata(data):\n",
        "  if data['description'].isnull().sum()>0:\n",
        "    data['description'] = data['description'].replace(np.nan,\"\") #Replacing the NaN values in the \"Description\" column with an empty string\n",
        "  data = data.sort_values('video_id').drop_duplicates(subset=['trending_date', 'title'], keep='last')\n",
        "  data = data.drop(columns=[\"publish_time\",\"category_id\"],axis=1)\n",
        "  return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bO_ZZl5wxq"
      },
      "source": [
        "##Loading the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "bNTt_yKxbf1O"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path): \n",
        "    chfile=file_path\n",
        "    codecountry = chfile[0:2]\n",
        "    path=os.path.join(dataset_path,chfile)\n",
        "    inputdf=pd.read_csv(path,encoding='utf-8')\n",
        "    titledict={}\n",
        "    pathjson=os.path.join(dataset_path,(codecountry+str('_category_id.json')))\n",
        "    jsoninputdf=pd.read_json(pathjson)\n",
        "    inputdf['region']=codecountry\n",
        "    print('HI')\n",
        "    for dict_a in jsoninputdf['items']:\n",
        "        titledict[dict_a['id']]=dict_a['snippet']['title'] \n",
        "    inputdf = dateprocessattributes(inputdf) #Calling the function to process the date columns.\n",
        "    inputdf['category']=inputdf['category_id'].apply(categmatch,args=(titledict,))\n",
        "    inputdf = cleandata(inputdf)\n",
        "    return inputdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "id": "vYhLfsK1fMDy",
        "outputId": "bb5fa477-6dfc-47e3-e206-0c99b7cb01df"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>\n",
              "  #output-body {\n",
              "    display: flex;\n",
              "    align-items: center;\n",
              "    justify-content: center;\n",
              "    background-color:white;\n",
              "    color:black;\n",
              "    }\n",
              "  .output_text pre{\n",
              "    margin-right : 70px;\n",
              "    color: black;\n",
              "    font-weight: bold;\n",
              "    /* font-size: 10px; */\n",
              "  }\n",
              "\n",
              "}  </style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "................................\n",
            "HI\n",
            "------Created CA DataFrame------\n",
            "HI\n",
            "------Created DE DataFrame------\n",
            "HI\n",
            "------Created FR DataFrame------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_33420\\1869322958.py:5: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  inputdf=pd.read_csv(path,encoding='utf-8')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HI\n",
            "------Created GB DataFrame------\n",
            "HI\n",
            "------Created IN DataFrame------\n",
            "HI\n",
            "------Created US DataFrame------\n",
            "Processed all available Datasets\n",
            "................................\n",
            "HI\n",
            "------Created CA DataFrame------\n",
            "HI\n",
            "------Created DE DataFrame------\n",
            "HI\n",
            "------Created FR DataFrame------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_33420\\1869322958.py:5: DtypeWarning: Columns (12) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  inputdf=pd.read_csv(path,encoding='utf-8')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HI\n",
            "------Created GB DataFrame------\n",
            "HI\n",
            "------Created IN DataFrame------\n",
            "Processed all available Datasets\n",
            "................................\n",
            "tdf  is the cumulative DataFrame\n"
          ]
        }
      ],
      "source": [
        "csspath()\n",
        "conlist = []\n",
        "condict = {}\n",
        "tlist = []\n",
        "t_df = pd.DataFrame()\n",
        "for dataset_pathname, _, filenames in os.walk(dataset_path):\n",
        "    print(\"................................\")\n",
        "    for paths in filenames:\n",
        "        if paths.lower().endswith(\".csv\"):\n",
        "          conlist.append(paths[0:2])\n",
        "          x = load_data(paths)\n",
        "          tlist.append(x)\n",
        "          x = x.drop(columns=['video_id','region'],axis=1)\n",
        "          globals()[str(paths[0:2])] = x\n",
        "          condict[str(paths[0:2])]=x\n",
        "          print(\"------Created\",paths[0:2],\"DataFrame------\")\n",
        "    print(\"Processed all available Datasets\")\n",
        "t_df = pd.concat(tlist,ignore_index=True)\n",
        "tdf = t_df\n",
        "tdf.name = \"The 6 Countries\"\n",
        "print(\"................................\")\n",
        "print(\"tdf  is the cumulative DataFrame\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EauET5Zs2jGU"
      },
      "outputs": [],
      "source": [
        "def getuniqcateg(data):\n",
        "  x_cat = data['category'].unique()\n",
        "  y_cat = list(x_cat)\n",
        "  if \"Unknown\" in y_cat:\n",
        "    y_cat.remove('Unknown')\n",
        "  cdict = {}\n",
        "  for i in range(0,len(y_cat)):\n",
        "    cdict[i] = y_cat[i]\n",
        "  return cdict,x_cat\n",
        "# dict_cat,x_cat = getuniqcateg(tdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF449RyLeq_H"
      },
      "source": [
        "##Filling the Missing Categories using Word Vectorisers and Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "7qFRrQoCdfv6",
        "outputId": "e13b813c-322b-4850-de0e-fb3aeddea2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CA 74\n",
            "DE 256\n",
            "FR 114\n",
            "GB 90\n",
            "IN 103\n",
            "US 0\n"
          ]
        }
      ],
      "source": [
        "for i,j in condict.items():\n",
        "  print(i,j[j.category==\"Unknown\"].value_counts().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "RZ7B9EhJmnzV"
      },
      "outputs": [],
      "source": [
        "gnb_model = MultinomialNB()\n",
        "rfc_model= RandomForestClassifier(n_estimators=20,verbose=False,n_jobs=-1)\n",
        "dtc_model = DecisionTreeClassifier()\n",
        "xgb_model=XGBClassifier()\n",
        "knn_model = KNeighborsClassifier(n_neighbors=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5oSRq7VufgKs"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\n\\nThis function create_model_data is designed to prepare data for training a machine learning model. It takes two parameters: data, \\nwhich presumably contains information about titles and their corresponding categories, and nam, which is a string used to name the vectorizer file.\\n\\n\\nIt extracts only the \\'title\\' and \\'category\\' columns from the input data.\\nIt removes any rows where the category is labeled as \"Unknown\" and resets the index.\\nIt assigns numerical category IDs to each category, making it easier for machine learning algorithms to process.\\nIt creates a CountVectorizer object to convert the text data (titles) into numerical vectors.\\nIt fits this vectorizer on the title data and transforms the titles into a matrix of word counts.\\nIt stores the fitted vectorizer object into a pickle file with the name provided by the nam parameter.\\nIt returns the word count matrix (words) and the numerical category IDs (cats) for further use in training the machine learning model\\n\\n\\n'"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def create_model_data(data,nam):\n",
        "  mdf = data[[\"title\",\"category\"]]\n",
        "  mdf = mdf[mdf['category']!=\"Unknown\"].reset_index()\n",
        "  mdf['cid'] = 0\n",
        "  dict_a,_ = getuniqcateg(data)\n",
        "  for index,i in enumerate(mdf.category):\n",
        "    for j in dict_a.keys():\n",
        "      if dict_a[j]==i:\n",
        "        mdf.at[index,'cid'] = j\n",
        "  v = CountVectorizer()\n",
        "  words = v.fit_transform(mdf['title'].values)\n",
        "  cats = mdf.cid.values\n",
        "  filename = nam + '_vector.pkl'\n",
        "  pickle.dump(v, open(filename, 'wb'))\n",
        "  return words,cats\n",
        "'''\n",
        "\n",
        "This function create_model_data is designed to prepare data for training a machine learning model. It takes two parameters: data, \n",
        "which presumably contains information about titles and their corresponding categories, and nam, which is a string used to name the vectorizer file.\n",
        "\n",
        "\n",
        "It extracts only the 'title' and 'category' columns from the input data.\n",
        "It removes any rows where the category is labeled as \"Unknown\" and resets the index.\n",
        "It assigns numerical category IDs to each category, making it easier for machine learning algorithms to process.\n",
        "It creates a CountVectorizer object to convert the text data (titles) into numerical vectors.\n",
        "It fits this vectorizer on the title data and transforms the titles into a matrix of word counts.\n",
        "It stores the fitted vectorizer object into a pickle file with the name provided by the nam parameter.\n",
        "It returns the word count matrix (words) and the numerical category IDs (cats) for further use in training the machine learning model\n",
        "\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LWm2mdm4xG7M"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nThis Python function, ensembleecreate, is designed to create an ensemble model using a combination of different classifiers. Here's a brief explanation of what it does:\\n\\nInput Parameters: The function takes two parameters: data, which represents the dataset, and nam, which is a string indicating the name of the model.\\nModel Training Data Preparation: Inside the function, create_model_data(data, nam) is called to prepare the feature matrix X and target vector y specific to the model indicated by nam.\\nModel Training: The function then splits the data into training and testing sets using train_test_split(). It trains individual classifiers (dtc_model, gnb_model, dtc_model, rfc_model, knn_model) on the entire dataset.\\nEnsemble Creation: An ensemble model (ens_name) is created using VotingClassifier from scikit-learn, which combines the predictions of the individual classifiers using majority voting (voting='hard').\\nModel Evaluation: The accuracy of the ensemble model is evaluated on the testing set using score() method.\\nModel Serialization: The trained ensemble model is saved to a file using pickle.dump() for future use.\\nReturn: Finally, the function returns the trained ensemble model.\\n\\n\""
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def ensembleecreate(data,nam):\n",
        "  print(\"-----------------Starting for\",nam,\"-----------------\")\n",
        "  X,y = create_model_data(data,nam)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2)\n",
        "  dtc = dtc_model.fit(X,y)\n",
        "  gnb = gnb_model.fit(X,y)\n",
        "  xgb = dtc_model.fit(X,y)\n",
        "  rfc = rfc_model.fit(X,y)\n",
        "\n",
        "  knn = knn_model.fit(X_train,y_train)\n",
        "  print(\"----------------------------------------\",\"Implementing Ensemble model has started\",\"----------------------------------------\")\n",
        "  estimators=[('gnb_model', gnb), ('xgb_model', xgb), ('dtc_model',dtc), ('knn_model', knn), ('rfc_model', rfc)]\n",
        "  # ens_name = \"ensemble_\" + str(nam)\n",
        "  ens_name = VotingClassifier(estimators, voting='hard')\n",
        "  ens_name.fit(X,y)\n",
        "  acc_ens = ens_name.score(X_test, y_test)\n",
        "  print(\"----------------------------------------\",\"Implementing Ensemble model has finished\",\"----------------------------------------\")\n",
        "  print(\"\\nThe Ensemble Model for %s has achieved an accuracy of %.4f\" %(nam,acc_ens))\n",
        "  filename = nam + '_model.pkl'\n",
        "  pickle.dump(ens_name, open(filename, 'wb'))\n",
        "  return ens_name  \n",
        "'''\n",
        "This Python function, ensembleecreate, is designed to create an ensemble model using a combination of different classifiers. Here's a brief explanation of what it does:\n",
        "\n",
        "Input Parameters: The function takes two parameters: data, which represents the dataset, and nam, which is a string indicating the name of the model.\n",
        "Model Training Data Preparation: Inside the function, create_model_data(data, nam) is called to prepare the feature matrix X and target vector y specific to the model indicated by nam.\n",
        "Model Training: The function then splits the data into training and testing sets using train_test_split(). It trains individual classifiers (dtc_model, gnb_model, dtc_model, rfc_model, knn_model) on the entire dataset.\n",
        "Ensemble Creation: An ensemble model (ens_name) is created using VotingClassifier from scikit-learn, which combines the predictions of the individual classifiers using majority voting (voting='hard').\n",
        "Model Evaluation: The accuracy of the ensemble model is evaluated on the testing set using score() method.\n",
        "Model Serialization: The trained ensemble model is saved to a file using pickle.dump() for future use.\n",
        "Return: Finally, the function returns the trained ensemble model.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "MecUqA4erg0m",
        "outputId": "2a46af49-730b-49ea-a833-cda004f0f86b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------Starting for CA -----------------\n",
            "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
            "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
            "\n",
            "The Ensemble Model for CA has achieved an accuracy of 0.9979\n",
            "-----------------Starting for DE -----------------\n",
            "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
            "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
            "\n",
            "The Ensemble Model for DE has achieved an accuracy of 0.9978\n",
            "-----------------Starting for FR -----------------\n",
            "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
            "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
            "\n",
            "The Ensemble Model for FR has achieved an accuracy of 0.9968\n",
            "-----------------Starting for GB -----------------\n",
            "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
            "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
            "\n",
            "The Ensemble Model for GB has achieved an accuracy of 0.9981\n",
            "-----------------Starting for IN -----------------\n",
            "---------------------------------------- Implementing Ensemble model has started ----------------------------------------\n",
            "---------------------------------------- Implementing Ensemble model has finished ----------------------------------------\n",
            "\n",
            "The Ensemble Model for IN has achieved an accuracy of 0.9989\n"
          ]
        }
      ],
      "source": [
        "def ensembleperform():\n",
        "  for i,j in condict.items():\n",
        "    if j[j.category==\"Unknown\"].value_counts().sum() > 0:\n",
        "      globals()[str(\"ensemble_\") + str(i)]  = ensembleecreate(j,i)\n",
        "ensembleperform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Y_ytZWio4R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "DA_category_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
